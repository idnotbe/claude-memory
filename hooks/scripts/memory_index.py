#!/usr/bin/env python3
"""
Memory index management utility for claude-memory plugin.

Rebuild, validate, query, health-check, or garbage-collect the index.md
file that serves as the lightweight retrieval layer for stored memories.

Usage:
  python memory_index.py --rebuild --root .claude/memory
  python memory_index.py --validate --root .claude/memory
  python memory_index.py --query "authentication" --root .claude/memory
  python memory_index.py --health --root .claude/memory
  python memory_index.py --gc --root .claude/memory

No external dependencies required (stdlib only).
"""

import argparse
import json
import os
import sys
from datetime import datetime, timezone
from pathlib import Path

# Category folder mapping
CATEGORY_FOLDERS = {
    "session_summary": "sessions",
    "decision": "decisions",
    "runbook": "runbooks",
    "constraint": "constraints",
    "tech_debt": "tech-debt",
    "preference": "preferences",
}

CATEGORY_DISPLAY = {
    "session_summary": "SESSION_SUMMARY",
    "decision": "DECISION",
    "runbook": "RUNBOOK",
    "constraint": "CONSTRAINT",
    "tech_debt": "TECH_DEBT",
    "preference": "PREFERENCE",
}


def scan_memories(root: Path, include_inactive: bool = False) -> list[dict]:
    """Scan all JSON files in category subfolders and extract metadata.

    Args:
        root: Memory root directory (.claude/memory)
        include_inactive: If True, include retired/archived entries.
                         If False (default), only include active entries.
    """
    memories = []
    for category, folder in CATEGORY_FOLDERS.items():
        folder_path = root / folder
        if not folder_path.is_dir():
            continue
        for json_file in sorted(folder_path.glob("*.json")):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                record_status = data.get("record_status", "active")

                # Skip non-active unless explicitly requested
                if not include_inactive and record_status != "active":
                    continue

                title = data.get("title", json_file.stem)
                cat = data.get("category", category)
                display = CATEGORY_DISPLAY.get(cat, cat.upper())
                tags = data.get("tags", [])
                rel_path = json_file.relative_to(root.parent.parent)
                memories.append({
                    "category": cat,
                    "display": display,
                    "title": title,
                    "path": str(rel_path).replace("\\", "/"),
                    "tags": tags,
                    "file": json_file,
                    "record_status": record_status,
                    "data": data,
                })
            except (json.JSONDecodeError, KeyError) as e:
                print(f"WARNING: Could not parse {json_file}: {e}", file=sys.stderr)
    return memories


def _sanitize_index_title(title: str) -> str:
    """Sanitize title for safe inclusion in index.md lines.

    Prevents injection of index-format markers ( -> , #tags:) and newlines
    that would corrupt line-based parsing.
    """
    # Collapse all whitespace (including newlines) to single spaces
    title = " ".join(title.split())
    title = title.replace(" -> ", " - ")
    title = title.replace("#tags:", "")
    return title[:120]


def rebuild_index(root: Path) -> None:
    """Scan all memory files and regenerate index.md with enriched format.

    Only indexes active memories. Includes #tags: suffix from JSON tags.
    """
    memories = scan_memories(root, include_inactive=False)
    if not memories:
        print("No active memory files found. Nothing to index.")
        return

    # Sort by category display name, then title
    memories.sort(key=lambda m: (m["display"], m["title"].lower()))

    lines = ["# Memory Index", "", "<!-- Auto-generated by memory_index.py. Do not edit manually. -->", ""]
    for m in memories:
        line = f"- [{m['display']}] {_sanitize_index_title(m['title'])} -> {m['path']}"
        # Append tags if present
        if m["tags"]:
            tags_str = ",".join(m["tags"])
            line += f" #tags:{tags_str}"
        lines.append(line)
    lines.append("")

    index_path = root / "index.md"
    with open(index_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Rebuilt index.md with {len(memories)} entries at {index_path}")


def validate_index(root: Path) -> bool:
    """Compare index.md entries against actual files and report mismatches."""
    index_path = root / "index.md"
    if not index_path.exists():
        print("ERROR: index.md does not exist. Run --rebuild first.")
        return False

    # Parse index entries
    indexed_paths = set()
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("- [") and " -> " in line:
                # Extract path, stripping any #tags: suffix
                after_arrow = line.split(" -> ", 1)[1]
                path_part = after_arrow.split(" #tags:")[0].strip()
                indexed_paths.add(path_part)

    # Scan actual active files
    actual_memories = scan_memories(root, include_inactive=False)
    actual_paths = {m["path"] for m in actual_memories}

    # Compare
    missing_from_index = actual_paths - indexed_paths
    stale_in_index = indexed_paths - actual_paths

    valid = True
    if missing_from_index:
        print("Files NOT in index.md:")
        for p in sorted(missing_from_index):
            print(f"  + {p}")
        valid = False

    if stale_in_index:
        print("Index entries with NO matching file:")
        for p in sorted(stale_in_index):
            print(f"  - {p}")
        valid = False

    if valid:
        print(f"Index is valid. {len(indexed_paths)} entries match {len(actual_paths)} files.")
    else:
        print(f"\nIndex has mismatches. Run --rebuild to fix.")

    return valid


def query_index(root: Path, query: str) -> None:
    """Search index entries by keyword."""
    index_path = root / "index.md"
    if not index_path.exists():
        print("ERROR: index.md does not exist. Run --rebuild first.")
        return

    query_lower = query.lower()
    matches = []
    with open(index_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("- [") and query_lower in line.lower():
                matches.append(line)

    if matches:
        print(f"Found {len(matches)} match(es) for '{query}':")
        for m in matches:
            print(f"  {m}")
    else:
        print(f"No matches found for '{query}'.")


def gc_retired(root: Path) -> None:
    """Garbage collect retired memories past the grace period.

    Reads config for delete.grace_period_days (default 30).
    Scans all category folders for retired files past the grace period.
    Deletes them permanently.
    """
    # Read config for grace period
    grace_period_days = 30
    config_path = root / "memory-config.json"
    if config_path.exists():
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
            raw_gpd = config.get("delete", {}).get("grace_period_days", 30)
            try:
                grace_period_days = max(0, int(raw_gpd))
            except (ValueError, TypeError):
                grace_period_days = 30
        except (json.JSONDecodeError, OSError):
            pass

    now = datetime.now(timezone.utc)
    deleted = []
    errors = []

    # Scan all category folders for retired files
    all_memories = scan_memories(root, include_inactive=True)
    for m in all_memories:
        if m["record_status"] != "retired":
            continue

        data = m["data"]
        retired_at_str = data.get("retired_at")
        if not retired_at_str:
            errors.append(f"  SKIP {m['file'].name}: missing retired_at timestamp")
            continue

        try:
            retired_at_str = retired_at_str.replace("Z", "+00:00")
            retired_at = datetime.fromisoformat(retired_at_str)
            if retired_at.tzinfo is None:
                retired_at = retired_at.replace(tzinfo=timezone.utc)
        except (ValueError, TypeError):
            errors.append(f"  SKIP {m['file'].name}: invalid retired_at timestamp")
            continue

        age_days = (now - retired_at).days
        if age_days >= grace_period_days:
            try:
                m["file"].unlink()
                deleted.append(f"  DELETED {m['file'].name} (retired {age_days} days ago)")
            except OSError as e:
                errors.append(f"  ERROR deleting {m['file'].name}: {e}")

    # Report
    print(f"Garbage Collection (grace period: {grace_period_days} days)")
    print(f"{'=' * 50}")
    if deleted:
        print(f"\nDeleted {len(deleted)} file(s):")
        for line in deleted:
            print(line)
    else:
        print("\nNo retired memories past grace period.")

    if errors:
        print(f"\nWarnings/errors ({len(errors)}):")
        for line in errors:
            print(line)

    # Suggest rebuild if files were deleted
    if deleted:
        print(f"\nRun --rebuild to update index.md.")


def health_report(root: Path) -> None:
    """Report statistics about the memory store.

    Includes:
    - Total entries by category
    - Heavily updated memories (times_updated > 5)
    - Recent retirements (last 7 days)
    - Index desync detection
    - Overall health summary
    """
    print("Memory Health Report")
    print("=" * 50)

    # Scan all memories (including inactive for full picture)
    all_memories = scan_memories(root, include_inactive=True)
    active_memories = [m for m in all_memories if m["record_status"] == "active"]
    retired_memories = [m for m in all_memories if m["record_status"] == "retired"]
    archived_memories = [m for m in all_memories if m["record_status"] == "archived"]

    # Total entries by category
    print(f"\n--- Entries by Category ---")
    cat_counts = {}
    for m in active_memories:
        cat_counts[m["display"]] = cat_counts.get(m["display"], 0) + 1
    if cat_counts:
        for cat in sorted(cat_counts.keys()):
            print(f"  {cat}: {cat_counts[cat]}")
        print(f"  TOTAL (active): {len(active_memories)}")
    else:
        print("  No active memories found.")

    if retired_memories:
        print(f"  Retired: {len(retired_memories)}")
    if archived_memories:
        print(f"  Archived: {len(archived_memories)}")

    # Heavily updated memories (times_updated > 5)
    print(f"\n--- Heavily Updated (times_updated > 5) ---")
    heavy = []
    for m in all_memories:
        times_updated = m["data"].get("times_updated", 0)
        if times_updated > 5:
            heavy.append((times_updated, m["title"], m["display"]))
    if heavy:
        heavy.sort(key=lambda x: -x[0])
        for count, title, cat in heavy:
            print(f"  [{cat}] {title} (updated {count} times)")
    else:
        print("  None.")

    # Recent retirements (last 7 days)
    print(f"\n--- Recent Retirements (last 7 days) ---")
    now = datetime.now(timezone.utc)
    recent_retired = []
    for m in retired_memories:
        retired_at_str = m["data"].get("retired_at")
        if not retired_at_str:
            continue
        try:
            retired_at_str = retired_at_str.replace("Z", "+00:00")
            retired_at = datetime.fromisoformat(retired_at_str)
            if retired_at.tzinfo is None:
                retired_at = retired_at.replace(tzinfo=timezone.utc)
            age_days = (now - retired_at).days
            if age_days <= 7:
                recent_retired.append((age_days, m["title"], m["display"]))
        except (ValueError, TypeError):
            continue
    if recent_retired:
        recent_retired.sort(key=lambda x: x[0])
        for age, title, cat in recent_retired:
            print(f"  [{cat}] {title} ({age} day(s) ago)")
    else:
        print("  None.")

    # Index desync detection
    print(f"\n--- Index Sync Status ---")
    index_path = root / "index.md"
    index_desynced = False
    if not index_path.exists():
        print("  WARNING: index.md does not exist!")
        index_desynced = True
    else:
        indexed_paths = set()
        try:
            with open(index_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("- [") and " -> " in line:
                        after_arrow = line.split(" -> ", 1)[1]
                        path_part = after_arrow.split(" #tags:")[0].strip()
                        indexed_paths.add(path_part)
        except OSError:
            print("  ERROR: Could not read index.md")
            indexed_paths = set()

        active_paths = {m["path"] for m in active_memories}
        missing_from_idx = active_paths - indexed_paths
        stale_in_idx = indexed_paths - active_paths

        if not missing_from_idx and not stale_in_idx:
            print(f"  Index is in sync ({len(indexed_paths)} entries).")
        else:
            index_desynced = True
            if missing_from_idx:
                print(f"  FILES MISSING FROM INDEX ({len(missing_from_idx)}):")
                for p in sorted(missing_from_idx):
                    print(f"    + {p}")
            if stale_in_idx:
                print(f"  STALE INDEX ENTRIES ({len(stale_in_idx)}):")
                for p in sorted(stale_in_idx):
                    print(f"    - {p}")
            print("  Run --rebuild to fix.")

    # Overall health summary
    print(f"\n--- Summary ---")
    issues = []
    if index_desynced:
        issues.append("Index out of sync or missing")
    if heavy:
        issues.append(f"{len(heavy)} heavily-updated memories (consider reviewing)")

    if not issues:
        print("  Health: GOOD")
    else:
        print(f"  Health: NEEDS ATTENTION")
        for issue in issues:
            print(f"    - {issue}")


def main():
    parser = argparse.ArgumentParser(
        description="Memory index management utility for claude-memory plugin."
    )
    parser.add_argument(
        "--root",
        type=str,
        default=".claude/memory",
        help="Root directory of memory storage (default: .claude/memory)",
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--rebuild",
        action="store_true",
        help="Scan all memory files and regenerate index.md",
    )
    group.add_argument(
        "--validate",
        action="store_true",
        help="Check index.md against actual files",
    )
    group.add_argument(
        "--query",
        type=str,
        metavar="KEYWORD",
        help="Search index entries by keyword",
    )
    group.add_argument(
        "--health",
        action="store_true",
        help="Report memory store statistics and health",
    )
    group.add_argument(
        "--gc",
        action="store_true",
        help="Garbage collect retired memories past grace period",
    )

    args = parser.parse_args()
    root = Path(args.root)

    if not root.is_dir():
        print(f"ERROR: Memory root '{root}' is not a directory.", file=sys.stderr)
        sys.exit(1)

    if args.rebuild:
        rebuild_index(root)
    elif args.validate:
        ok = validate_index(root)
        sys.exit(0 if ok else 1)
    elif args.query:
        query_index(root, args.query)
    elif args.health:
        health_report(root)
    elif args.gc:
        gc_retired(root)


if __name__ == "__main__":
    main()
